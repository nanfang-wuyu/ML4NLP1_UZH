{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwwQqWtxaMWs"
      },
      "source": [
        "# ML4NLP1\n",
        "## Starting Point for Exercise 1, part I\n",
        "\n",
        "This notebook is supposed to serve as a starting point and/or inspiration when starting exercise 1, part I.\n",
        "\n",
        "One of the goals of this exercise is o make you acquainted with sklearn and related libraries like pandas and numpy. You will probably need to consult the documentation of those libraries:\n",
        "- sklearn: [Documentation](https://scikit-learn.org/stable/user_guide.html)\n",
        "- Pandas: [Documentation](https://pandas.pydata.org/docs/#)\n",
        "- Numpy: [Documentation](https://numpy.org/doc/)\n",
        "\n",
        "**Importing files to Google Colab:** If you have never used Colab or never uploaded a file to Colab, quickly skim over an introduction: [Introduction on medium](https://medium.com/@master_yi/importing-datasets-in-google-colab-c816fc654f97).\n",
        "\n",
        "We're using the second method mentioned in the blogpost: (1) upload the four files `x_train.txt` and `y_train.txt`, `x_test.txt` and `y_test.txt` to a directory in Google Drive and (2) adjust the paths in the second cell to point to your uploaded files.\n",
        "\n",
        "Then execute the first cell to give Colab permission to access the two files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kBqq-G2daMW7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import re\n",
        "import string\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEqfrri6aMW8",
        "outputId": "548476c8-e116-406f-e0de-59c8bfd63dcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QP6YuwdKFNUPpvhOaAcvv2Pcp4JMbIRs\n",
            "To: /content/x_train.txt\n",
            "100% 64.1M/64.1M [00:00<00:00, 202MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QVo7PZAdiZKzifK8kwhEr_umosiDCUx6\n",
            "To: /content/x_test.txt\n",
            "100% 65.2M/65.2M [00:00<00:00, 157MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QbBeKcmG2ZyAEFB3AKGTgSWQ1YEMn2jl\n",
            "To: /content/y_train.txt\n",
            "100% 480k/480k [00:00<00:00, 132MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QaZj6bI7_78ymnN8IpSk4gVvg-C9fA6X\n",
            "To: /content/y_test.txt\n",
            "100% 480k/480k [00:00<00:00, 114MB/s]\n"
          ]
        }
      ],
      "source": [
        "# download dataset\n",
        "!gdown 1QP6YuwdKFNUPpvhOaAcvv2Pcp4JMbIRs # x_train\n",
        "!gdown 1QVo7PZAdiZKzifK8kwhEr_umosiDCUx6 # x_test\n",
        "!gdown 1QbBeKcmG2ZyAEFB3AKGTgSWQ1YEMn2jl # y_train\n",
        "!gdown 1QaZj6bI7_78ymnN8IpSk4gVvg-C9fA6X # y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "oU9VZeEEabMQ"
      },
      "outputs": [],
      "source": [
        "with open(f'x_train.txt') as f:\n",
        "    x_train = f.read().splitlines()\n",
        "with open(f'y_train.txt') as f:\n",
        "    y_train = f.read().splitlines()\n",
        "with open(f'x_test.txt') as f:\n",
        "    x_test = f.read().splitlines()\n",
        "with open(f'y_test.txt') as f:\n",
        "    y_test = f.read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "n4PijB0TaMW8"
      },
      "outputs": [],
      "source": [
        "# combine x_train and y_train into one dataframe\n",
        "train_df = pd.DataFrame({'text': x_train, 'label': y_train})\n",
        "# write train_df to csv with tab as separator\n",
        "train_df.to_csv('train_df.csv', index=False, sep='\\t')\n",
        "# comibne x_test and y_test into one dataframe\n",
        "test_df = pd.DataFrame({'text': x_test, 'label': y_test})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-TjW9Z2OytMV",
        "outputId": "b4fa6cae-9f56-4490-c5ba-4ee79b6721ff"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Klement Gottwaldi surnukeha palsameeriti ning ...</td>\n",
              "      <td>est</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sebes, Joseph; Pereira Thomas (1961) (på eng)....</td>\n",
              "      <td>swe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>भारतीय स्वातन्त्र्य आन्दोलन राष्ट्रीय एवम क्षे...</td>\n",
              "      <td>mai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Après lo cort periòde d'establiment a Basilèa,...</td>\n",
              "      <td>oci</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ถนนเจริญกรุง (อักษรโรมัน: Thanon Charoen Krung...</td>\n",
              "      <td>tha</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text label\n",
              "0  Klement Gottwaldi surnukeha palsameeriti ning ...   est\n",
              "1  Sebes, Joseph; Pereira Thomas (1961) (på eng)....   swe\n",
              "2  भारतीय स्वातन्त्र्य आन्दोलन राष्ट्रीय एवम क्षे...   mai\n",
              "3  Après lo cort periòde d'establiment a Basilèa,...   oci\n",
              "4  ถนนเจริญกรุง (อักษรโรมัน: Thanon Charoen Krung...   tha"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_96MFk6daMW9",
        "outputId": "1e0800ac-9605-4a9b-d2c1-cf50e14fbe90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['est', 'swe', 'mai', 'oci', 'tha', 'orm', 'lim', 'guj', 'pnb', 'zea', 'krc', 'hat', 'pcd', 'tam', 'vie', 'pan', 'szl', 'ckb', 'fur', 'wuu', 'arz', 'ton', 'eus', 'map-bms', 'glk', 'nld', 'bod', 'jpn', 'arg', 'srd', 'ext', 'sin', 'kur', 'che', 'tuk', 'pag', 'tur', 'als', 'koi', 'lat', 'urd', 'tat', 'bxr', 'ind', 'kir', 'zh-yue', 'dan', 'por', 'fra', 'ori', 'nob', 'jbo', 'kok', 'amh', 'khm', 'hbs', 'slv', 'bos', 'tet', 'zho', 'kor', 'sah', 'rup', 'ast', 'wol', 'bul', 'gla', 'msa', 'crh', 'lug', 'sun', 'bre', 'mon', 'nep', 'ibo', 'cdo', 'asm', 'grn', 'hin', 'mar', 'lin', 'ile', 'lmo', 'mya', 'ilo', 'csb', 'tyv', 'gle', 'nan', 'jam', 'scn', 'be-tarask', 'diq', 'cor', 'fao', 'mlg', 'yid', 'sme', 'spa', 'kbd', 'udm', 'isl', 'ksh', 'san', 'aze', 'nap', 'dsb', 'pam', 'cym', 'srp', 'stq', 'tel', 'swa', 'vls', 'mzn', 'bel', 'lad', 'ina', 'ava', 'lao', 'min', 'ita', 'nds-nl', 'oss', 'kab', 'pus', 'fin', 'snd', 'kaa', 'fas', 'cbk', 'cat', 'nci', 'mhr', 'roa-tara', 'frp', 'ron', 'new', 'bar', 'ltg', 'vro', 'lav', 'ces', 'yor', 'nso', 'bak', 'rus', 'ace', 'mdf', 'vep', 'sgs', 'uig', 'lit', 'sqi', 'som', 'slk', 'sco', 'ukr', 'mri', 'hrv', 'vol', 'eng', 'glv', 'ben', 'ido', 'jav', 'tcy', 'mrj', 'hif', 'sna', 'war', 'mlt', 'egl', 'tsn', 'lez', 'hak', 'kom', 'azb', 'que', 'lzh', 'ara', 'fry', 'dty', 'mal', 'heb', 'gag', 'chv', 'afr', 'pap', 'kin', 'ang', 'kaz', 'bjn', 'hye', 'olo', 'xmf', 'uzb', 'bcl', 'pdc', 'hsb', 'srn', 'kat', 'tgl', 'nno', 'glg', 'roh', 'kan', 'chr', 'lrc', 'div', 'myv', 'cos', 'hun', 'nrm', 'pfl', 'wln', 'bho', 'epo', 'deu', 'nav', 'mwl', 'pol', 'rue', 'vec', 'nds', 'aym', 'tgk', 'ceb', 'xho', 'bpy', 'ell', 'lij', 'hau', 'mkd', 'ltz']\n"
          ]
        }
      ],
      "source": [
        "# get list of all labels\n",
        "labels = train_df['label'].unique().tolist()\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "235"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrDiqt-CaMW-"
      },
      "outputs": [],
      "source": [
        "# T: Have a quick peek at the training data, looking at a couple of texts from different languages. Do you notice anything that might be challenging for the classification?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Several languages in the Latin family use the same alphabet, and the words they form are extremely similar。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOQg_jmnaMW-"
      },
      "outputs": [],
      "source": [
        "# T: How many instances per label are there in the training and test set? Do you think this is a balanced dataset? Do you think the train/test split is appropriate? If not, please rearrange the data in a more appropriate way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ace': 500,\n",
              " 'afr': 500,\n",
              " 'als': 500,\n",
              " 'amh': 500,\n",
              " 'ang': 500,\n",
              " 'ara': 500,\n",
              " 'arg': 500,\n",
              " 'arz': 500,\n",
              " 'asm': 500,\n",
              " 'ast': 500,\n",
              " 'ava': 500,\n",
              " 'aym': 500,\n",
              " 'azb': 500,\n",
              " 'aze': 500,\n",
              " 'bak': 500,\n",
              " 'bar': 500,\n",
              " 'bcl': 500,\n",
              " 'be-tarask': 500,\n",
              " 'bel': 500,\n",
              " 'ben': 500,\n",
              " 'bho': 500,\n",
              " 'bjn': 500,\n",
              " 'bod': 500,\n",
              " 'bos': 500,\n",
              " 'bpy': 500,\n",
              " 'bre': 500,\n",
              " 'bul': 500,\n",
              " 'bxr': 500,\n",
              " 'cat': 500,\n",
              " 'cbk': 500,\n",
              " 'cdo': 500,\n",
              " 'ceb': 500,\n",
              " 'ces': 500,\n",
              " 'che': 500,\n",
              " 'chr': 500,\n",
              " 'chv': 500,\n",
              " 'ckb': 500,\n",
              " 'cor': 500,\n",
              " 'cos': 500,\n",
              " 'crh': 500,\n",
              " 'csb': 500,\n",
              " 'cym': 500,\n",
              " 'dan': 500,\n",
              " 'deu': 500,\n",
              " 'diq': 500,\n",
              " 'div': 500,\n",
              " 'dsb': 500,\n",
              " 'dty': 500,\n",
              " 'egl': 500,\n",
              " 'ell': 500,\n",
              " 'eng': 500,\n",
              " 'epo': 500,\n",
              " 'est': 500,\n",
              " 'eus': 500,\n",
              " 'ext': 500,\n",
              " 'fao': 500,\n",
              " 'fas': 500,\n",
              " 'fin': 500,\n",
              " 'fra': 500,\n",
              " 'frp': 500,\n",
              " 'fry': 500,\n",
              " 'fur': 500,\n",
              " 'gag': 500,\n",
              " 'gla': 500,\n",
              " 'gle': 500,\n",
              " 'glg': 500,\n",
              " 'glk': 500,\n",
              " 'glv': 500,\n",
              " 'grn': 500,\n",
              " 'guj': 500,\n",
              " 'hak': 500,\n",
              " 'hat': 500,\n",
              " 'hau': 500,\n",
              " 'hbs': 500,\n",
              " 'heb': 500,\n",
              " 'hif': 500,\n",
              " 'hin': 500,\n",
              " 'hrv': 500,\n",
              " 'hsb': 500,\n",
              " 'hun': 500,\n",
              " 'hye': 500,\n",
              " 'ibo': 500,\n",
              " 'ido': 500,\n",
              " 'ile': 500,\n",
              " 'ilo': 500,\n",
              " 'ina': 500,\n",
              " 'ind': 500,\n",
              " 'isl': 500,\n",
              " 'ita': 500,\n",
              " 'jam': 500,\n",
              " 'jav': 500,\n",
              " 'jbo': 500,\n",
              " 'jpn': 500,\n",
              " 'kaa': 500,\n",
              " 'kab': 500,\n",
              " 'kan': 500,\n",
              " 'kat': 500,\n",
              " 'kaz': 500,\n",
              " 'kbd': 500,\n",
              " 'khm': 500,\n",
              " 'kin': 500,\n",
              " 'kir': 500,\n",
              " 'koi': 500,\n",
              " 'kok': 500,\n",
              " 'kom': 500,\n",
              " 'kor': 500,\n",
              " 'krc': 500,\n",
              " 'ksh': 500,\n",
              " 'kur': 500,\n",
              " 'lad': 500,\n",
              " 'lao': 500,\n",
              " 'lat': 500,\n",
              " 'lav': 500,\n",
              " 'lez': 500,\n",
              " 'lij': 500,\n",
              " 'lim': 500,\n",
              " 'lin': 500,\n",
              " 'lit': 500,\n",
              " 'lmo': 500,\n",
              " 'lrc': 500,\n",
              " 'ltg': 500,\n",
              " 'ltz': 500,\n",
              " 'lug': 500,\n",
              " 'lzh': 500,\n",
              " 'mai': 500,\n",
              " 'mal': 500,\n",
              " 'map-bms': 500,\n",
              " 'mar': 500,\n",
              " 'mdf': 500,\n",
              " 'mhr': 500,\n",
              " 'min': 500,\n",
              " 'mkd': 500,\n",
              " 'mlg': 500,\n",
              " 'mlt': 500,\n",
              " 'mon': 500,\n",
              " 'mri': 500,\n",
              " 'mrj': 500,\n",
              " 'msa': 500,\n",
              " 'mwl': 500,\n",
              " 'mya': 500,\n",
              " 'myv': 500,\n",
              " 'mzn': 500,\n",
              " 'nan': 500,\n",
              " 'nap': 500,\n",
              " 'nav': 500,\n",
              " 'nci': 500,\n",
              " 'nds': 500,\n",
              " 'nds-nl': 500,\n",
              " 'nep': 500,\n",
              " 'new': 500,\n",
              " 'nld': 500,\n",
              " 'nno': 500,\n",
              " 'nob': 500,\n",
              " 'nrm': 500,\n",
              " 'nso': 500,\n",
              " 'oci': 500,\n",
              " 'olo': 500,\n",
              " 'ori': 500,\n",
              " 'orm': 500,\n",
              " 'oss': 500,\n",
              " 'pag': 500,\n",
              " 'pam': 500,\n",
              " 'pan': 500,\n",
              " 'pap': 500,\n",
              " 'pcd': 500,\n",
              " 'pdc': 500,\n",
              " 'pfl': 500,\n",
              " 'pnb': 500,\n",
              " 'pol': 500,\n",
              " 'por': 500,\n",
              " 'pus': 500,\n",
              " 'que': 500,\n",
              " 'roa-tara': 500,\n",
              " 'roh': 500,\n",
              " 'ron': 500,\n",
              " 'rue': 500,\n",
              " 'rup': 500,\n",
              " 'rus': 500,\n",
              " 'sah': 500,\n",
              " 'san': 500,\n",
              " 'scn': 500,\n",
              " 'sco': 500,\n",
              " 'sgs': 500,\n",
              " 'sin': 500,\n",
              " 'slk': 500,\n",
              " 'slv': 500,\n",
              " 'sme': 500,\n",
              " 'sna': 500,\n",
              " 'snd': 500,\n",
              " 'som': 500,\n",
              " 'spa': 500,\n",
              " 'sqi': 500,\n",
              " 'srd': 500,\n",
              " 'srn': 500,\n",
              " 'srp': 500,\n",
              " 'stq': 500,\n",
              " 'sun': 500,\n",
              " 'swa': 500,\n",
              " 'swe': 500,\n",
              " 'szl': 500,\n",
              " 'tam': 500,\n",
              " 'tat': 500,\n",
              " 'tcy': 500,\n",
              " 'tel': 500,\n",
              " 'tet': 500,\n",
              " 'tgk': 500,\n",
              " 'tgl': 500,\n",
              " 'tha': 500,\n",
              " 'ton': 500,\n",
              " 'tsn': 500,\n",
              " 'tuk': 500,\n",
              " 'tur': 500,\n",
              " 'tyv': 500,\n",
              " 'udm': 500,\n",
              " 'uig': 500,\n",
              " 'ukr': 500,\n",
              " 'urd': 500,\n",
              " 'uzb': 500,\n",
              " 'vec': 500,\n",
              " 'vep': 500,\n",
              " 'vie': 500,\n",
              " 'vls': 500,\n",
              " 'vol': 500,\n",
              " 'vro': 500,\n",
              " 'war': 500,\n",
              " 'wln': 500,\n",
              " 'wol': 500,\n",
              " 'wuu': 500,\n",
              " 'xho': 500,\n",
              " 'xmf': 500,\n",
              " 'yid': 500,\n",
              " 'yor': 500,\n",
              " 'zea': 500,\n",
              " 'zh-yue': 500,\n",
              " 'zho': 500}"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# T: How many instances per label are there in the training and test set?\n",
        "labels, counts = np.unique(train_df['label'], return_counts=True)\n",
        "dict(zip(labels, counts))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ace': 500,\n",
              " 'afr': 500,\n",
              " 'als': 500,\n",
              " 'amh': 500,\n",
              " 'ang': 500,\n",
              " 'ara': 500,\n",
              " 'arg': 500,\n",
              " 'arz': 500,\n",
              " 'asm': 500,\n",
              " 'ast': 500,\n",
              " 'ava': 500,\n",
              " 'aym': 500,\n",
              " 'azb': 500,\n",
              " 'aze': 500,\n",
              " 'bak': 500,\n",
              " 'bar': 500,\n",
              " 'bcl': 500,\n",
              " 'be-tarask': 500,\n",
              " 'bel': 500,\n",
              " 'ben': 500,\n",
              " 'bho': 500,\n",
              " 'bjn': 500,\n",
              " 'bod': 500,\n",
              " 'bos': 500,\n",
              " 'bpy': 500,\n",
              " 'bre': 500,\n",
              " 'bul': 500,\n",
              " 'bxr': 500,\n",
              " 'cat': 500,\n",
              " 'cbk': 500,\n",
              " 'cdo': 500,\n",
              " 'ceb': 500,\n",
              " 'ces': 500,\n",
              " 'che': 500,\n",
              " 'chr': 500,\n",
              " 'chv': 500,\n",
              " 'ckb': 500,\n",
              " 'cor': 500,\n",
              " 'cos': 500,\n",
              " 'crh': 500,\n",
              " 'csb': 500,\n",
              " 'cym': 500,\n",
              " 'dan': 500,\n",
              " 'deu': 500,\n",
              " 'diq': 500,\n",
              " 'div': 500,\n",
              " 'dsb': 500,\n",
              " 'dty': 500,\n",
              " 'egl': 500,\n",
              " 'ell': 500,\n",
              " 'eng': 500,\n",
              " 'epo': 500,\n",
              " 'est': 500,\n",
              " 'eus': 500,\n",
              " 'ext': 500,\n",
              " 'fao': 500,\n",
              " 'fas': 500,\n",
              " 'fin': 500,\n",
              " 'fra': 500,\n",
              " 'frp': 500,\n",
              " 'fry': 500,\n",
              " 'fur': 500,\n",
              " 'gag': 500,\n",
              " 'gla': 500,\n",
              " 'gle': 500,\n",
              " 'glg': 500,\n",
              " 'glk': 500,\n",
              " 'glv': 500,\n",
              " 'grn': 500,\n",
              " 'guj': 500,\n",
              " 'hak': 500,\n",
              " 'hat': 500,\n",
              " 'hau': 500,\n",
              " 'hbs': 500,\n",
              " 'heb': 500,\n",
              " 'hif': 500,\n",
              " 'hin': 500,\n",
              " 'hrv': 500,\n",
              " 'hsb': 500,\n",
              " 'hun': 500,\n",
              " 'hye': 500,\n",
              " 'ibo': 500,\n",
              " 'ido': 500,\n",
              " 'ile': 500,\n",
              " 'ilo': 500,\n",
              " 'ina': 500,\n",
              " 'ind': 500,\n",
              " 'isl': 500,\n",
              " 'ita': 500,\n",
              " 'jam': 500,\n",
              " 'jav': 500,\n",
              " 'jbo': 500,\n",
              " 'jpn': 500,\n",
              " 'kaa': 500,\n",
              " 'kab': 500,\n",
              " 'kan': 500,\n",
              " 'kat': 500,\n",
              " 'kaz': 500,\n",
              " 'kbd': 500,\n",
              " 'khm': 500,\n",
              " 'kin': 500,\n",
              " 'kir': 500,\n",
              " 'koi': 500,\n",
              " 'kok': 500,\n",
              " 'kom': 500,\n",
              " 'kor': 500,\n",
              " 'krc': 500,\n",
              " 'ksh': 500,\n",
              " 'kur': 500,\n",
              " 'lad': 500,\n",
              " 'lao': 500,\n",
              " 'lat': 500,\n",
              " 'lav': 500,\n",
              " 'lez': 500,\n",
              " 'lij': 500,\n",
              " 'lim': 500,\n",
              " 'lin': 500,\n",
              " 'lit': 500,\n",
              " 'lmo': 500,\n",
              " 'lrc': 500,\n",
              " 'ltg': 500,\n",
              " 'ltz': 500,\n",
              " 'lug': 500,\n",
              " 'lzh': 500,\n",
              " 'mai': 500,\n",
              " 'mal': 500,\n",
              " 'map-bms': 500,\n",
              " 'mar': 500,\n",
              " 'mdf': 500,\n",
              " 'mhr': 500,\n",
              " 'min': 500,\n",
              " 'mkd': 500,\n",
              " 'mlg': 500,\n",
              " 'mlt': 500,\n",
              " 'mon': 500,\n",
              " 'mri': 500,\n",
              " 'mrj': 500,\n",
              " 'msa': 500,\n",
              " 'mwl': 500,\n",
              " 'mya': 500,\n",
              " 'myv': 500,\n",
              " 'mzn': 500,\n",
              " 'nan': 500,\n",
              " 'nap': 500,\n",
              " 'nav': 500,\n",
              " 'nci': 500,\n",
              " 'nds': 500,\n",
              " 'nds-nl': 500,\n",
              " 'nep': 500,\n",
              " 'new': 500,\n",
              " 'nld': 500,\n",
              " 'nno': 500,\n",
              " 'nob': 500,\n",
              " 'nrm': 500,\n",
              " 'nso': 500,\n",
              " 'oci': 500,\n",
              " 'olo': 500,\n",
              " 'ori': 500,\n",
              " 'orm': 500,\n",
              " 'oss': 500,\n",
              " 'pag': 500,\n",
              " 'pam': 500,\n",
              " 'pan': 500,\n",
              " 'pap': 500,\n",
              " 'pcd': 500,\n",
              " 'pdc': 500,\n",
              " 'pfl': 500,\n",
              " 'pnb': 500,\n",
              " 'pol': 500,\n",
              " 'por': 500,\n",
              " 'pus': 500,\n",
              " 'que': 500,\n",
              " 'roa-tara': 500,\n",
              " 'roh': 500,\n",
              " 'ron': 500,\n",
              " 'rue': 500,\n",
              " 'rup': 500,\n",
              " 'rus': 500,\n",
              " 'sah': 500,\n",
              " 'san': 500,\n",
              " 'scn': 500,\n",
              " 'sco': 500,\n",
              " 'sgs': 500,\n",
              " 'sin': 500,\n",
              " 'slk': 500,\n",
              " 'slv': 500,\n",
              " 'sme': 500,\n",
              " 'sna': 500,\n",
              " 'snd': 500,\n",
              " 'som': 500,\n",
              " 'spa': 500,\n",
              " 'sqi': 500,\n",
              " 'srd': 500,\n",
              " 'srn': 500,\n",
              " 'srp': 500,\n",
              " 'stq': 500,\n",
              " 'sun': 500,\n",
              " 'swa': 500,\n",
              " 'swe': 500,\n",
              " 'szl': 500,\n",
              " 'tam': 500,\n",
              " 'tat': 500,\n",
              " 'tcy': 500,\n",
              " 'tel': 500,\n",
              " 'tet': 500,\n",
              " 'tgk': 500,\n",
              " 'tgl': 500,\n",
              " 'tha': 500,\n",
              " 'ton': 500,\n",
              " 'tsn': 500,\n",
              " 'tuk': 500,\n",
              " 'tur': 500,\n",
              " 'tyv': 500,\n",
              " 'udm': 500,\n",
              " 'uig': 500,\n",
              " 'ukr': 500,\n",
              " 'urd': 500,\n",
              " 'uzb': 500,\n",
              " 'vec': 500,\n",
              " 'vep': 500,\n",
              " 'vie': 500,\n",
              " 'vls': 500,\n",
              " 'vol': 500,\n",
              " 'vro': 500,\n",
              " 'war': 500,\n",
              " 'wln': 500,\n",
              " 'wol': 500,\n",
              " 'wuu': 500,\n",
              " 'xho': 500,\n",
              " 'xmf': 500,\n",
              " 'yid': 500,\n",
              " 'yor': 500,\n",
              " 'zea': 500,\n",
              " 'zh-yue': 500,\n",
              " 'zho': 500}"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# T: How many instances per label are there in the training and test set?\n",
        "labels, counts = np.unique(test_df['label'], return_counts=True)\n",
        "dict(zip(labels, counts))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. T: How many instances per label are there in the training and test set?\n",
        "   \n",
        "   Answer: 500 instances per label in training set and 500 instances per label in test set. \n",
        "   \n",
        "2. Do you think this is a balanced dataset? \n",
        "\n",
        "   Answer: To some extend, yes, each language has the same number of instances. However, training set should contain more instances than test set.\n",
        "\n",
        "3. Do you think the train/test split is appropriate? \n",
        "   \n",
        "   Answer: No. Training set should have more instances each language than test set. Like 4:1. Here it's 1:1. \n",
        "\n",
        "4. If not, please rearrange the data in a more appropriate way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_set = pd.concat([train_df, test_df])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_train_df = pd.DataFrame()\n",
        "new_test_df = pd.DataFrame()\n",
        "for label in labels:\n",
        "    label_instances = data_set[data_set['label'] == label]\n",
        "    train_instances = label_instances.iloc[:800]\n",
        "    test_instances = label_instances.iloc[800:]\n",
        "    new_train_df = new_train_df.append(train_instances)\n",
        "    new_test_df = new_test_df.append(test_instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(235,\n",
              " array([800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
              "        800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
              "        800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
              "        800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
              "        800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
              "        800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
              "        800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
              "        800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
              "        800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
              "        800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
              "        800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
              "        800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
              "        800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
              "        800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
              "        800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
              "        800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
              "        800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
              "        800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800,\n",
              "        800]))"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels, counts = np.unique(new_train_df['label'], return_counts=True)\n",
        "len(labels), counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(235,\n",
              " array([200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
              "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
              "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
              "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
              "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
              "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
              "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
              "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
              "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
              "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
              "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
              "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
              "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
              "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
              "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
              "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
              "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
              "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
              "        200]))"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels, counts = np.unique(new_test_df['label'], return_counts=True)\n",
        "len(labels), counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = new_train_df.reset_index(drop=True)\n",
        "test_df = new_test_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sROq2PaJaMW-"
      },
      "outputs": [],
      "source": [
        "# T: Get a subset of the train/test data that includes English, German, Dutch, Danish, Swedish and Norwegian, plus 20 additional languages of your choice (the labels can be found in the file labels.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['ace', 'afr', 'als', 'amh', 'ang', 'ara', 'arg', 'arz', 'asm',\n",
              "       'ast', 'ava', 'aym', 'azb', 'aze', 'bak', 'bar', 'bcl',\n",
              "       'be-tarask', 'bel', 'ben', 'bho', 'bjn', 'bod', 'bos', 'bpy',\n",
              "       'bre', 'bul', 'bxr', 'cat', 'cbk', 'cdo', 'ceb', 'ces', 'che',\n",
              "       'chr', 'chv', 'ckb', 'cor', 'cos', 'crh', 'csb', 'cym', 'dan',\n",
              "       'deu', 'diq', 'div', 'dsb', 'dty', 'egl', 'ell', 'eng', 'epo',\n",
              "       'est', 'eus', 'ext', 'fao', 'fas', 'fin', 'fra', 'frp', 'fry',\n",
              "       'fur', 'gag', 'gla', 'gle', 'glg', 'glk', 'glv', 'grn', 'guj',\n",
              "       'hak', 'hat', 'hau', 'hbs', 'heb', 'hif', 'hin', 'hrv', 'hsb',\n",
              "       'hun', 'hye', 'ibo', 'ido', 'ile', 'ilo', 'ina', 'ind', 'isl',\n",
              "       'ita', 'jam', 'jav', 'jbo', 'jpn', 'kaa', 'kab', 'kan', 'kat',\n",
              "       'kaz', 'kbd', 'khm', 'kin', 'kir', 'koi', 'kok', 'kom', 'kor',\n",
              "       'krc', 'ksh', 'kur', 'lad', 'lao', 'lat', 'lav', 'lez', 'lij',\n",
              "       'lim', 'lin', 'lit', 'lmo', 'lrc', 'ltg', 'ltz', 'lug', 'lzh',\n",
              "       'mai', 'mal', 'map-bms', 'mar', 'mdf', 'mhr', 'min', 'mkd', 'mlg',\n",
              "       'mlt', 'mon', 'mri', 'mrj', 'msa', 'mwl', 'mya', 'myv', 'mzn',\n",
              "       'nan', 'nap', 'nav', 'nci', 'nds', 'nds-nl', 'nep', 'new', 'nld',\n",
              "       'nno', 'nob', 'nrm', 'nso', 'oci', 'olo', 'ori', 'orm', 'oss',\n",
              "       'pag', 'pam', 'pan', 'pap', 'pcd', 'pdc', 'pfl', 'pnb', 'pol',\n",
              "       'por', 'pus', 'que', 'roa-tara', 'roh', 'ron', 'rue', 'rup', 'rus',\n",
              "       'sah', 'san', 'scn', 'sco', 'sgs', 'sin', 'slk', 'slv', 'sme',\n",
              "       'sna', 'snd', 'som', 'spa', 'sqi', 'srd', 'srn', 'srp', 'stq',\n",
              "       'sun', 'swa', 'swe', 'szl', 'tam', 'tat', 'tcy', 'tel', 'tet',\n",
              "       'tgk', 'tgl', 'tha', 'ton', 'tsn', 'tuk', 'tur', 'tyv', 'udm',\n",
              "       'uig', 'ukr', 'urd', 'uzb', 'vec', 'vep', 'vie', 'vls', 'vol',\n",
              "       'vro', 'war', 'wln', 'wol', 'wuu', 'xho', 'xmf', 'yid', 'yor',\n",
              "       'zea', 'zh-yue', 'zho'], dtype=object)"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 344,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" \n",
        "English: eng\n",
        "German: deu\n",
        "Dutch: nld\n",
        "Danish: dan\n",
        "Swedish: swe\n",
        "Norwegian: nob\n",
        "\n",
        "Japanese: jpn\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "subset_labels = {'eng', 'deu', 'nld', 'dan', 'swe', 'jpn', 'nob'}\n",
        "other_labels = set(labels) - subset_labels\n",
        "subset_labels = subset_labels | set(list(other_labels)[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 345,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "execution_count": 345,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(subset_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 346,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(21600, 5400)"
            ]
          },
          "execution_count": 346,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sub_train_set = train_df[train_df['label'].isin(subset_labels)]\n",
        "sub_test_set = test_df[test_df['label'].isin(subset_labels)]\n",
        "len(sub_train_set), len(sub_test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 347,
      "metadata": {
        "id": "dOseJ2BsaMW_"
      },
      "outputs": [],
      "source": [
        "# T: With the following code, we wanted to encode the labels, \n",
        "# however, our cat was walking on the keyboard and some of it got changed. Can you fix it?\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "le_fitted = label_encoder.fit(sub_train_set['label'])\n",
        "y_train_dev, y_test = le_fitted.transform(sub_train_set['label']), le_fitted.transform(sub_test_set['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline\n",
        "Create a suitable pipeline in sklearn to preprocess the data. Think about extending the feature space.\n",
        "What other features could you use to determine the language? Please include additional linguistic\n",
        "features to your machine learning model for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 348,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Here we use TfidfVectorizer to replace CountVectorizer and TfidfTransformer\n",
        "# Due to Eli5, if we use TfidfTransformer there would be errors. \n",
        "# This is also more clear and intuitive. \n",
        "text_clf = Pipeline([\n",
        "    ('TfidfVect', TfidfVectorizer(ngram_range=(1, 2),)),\n",
        "    ('LR', LogisticRegression())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 349,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('TfidfVect', TfidfVectorizer()), ('LR', LogisticRegression())])"
            ]
          },
          "execution_count": 349,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_clf.fit(sub_train_set['text'], y_train_dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores = cross_val_score(text_clf, sub_train_set['text'], y_train_dev, scoring='accuracy', cv=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 351,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.96125   , 0.96013889, 0.96333333])"
            ]
          },
          "execution_count": 351,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters\n",
        "To find the optimal hyperparameter settings for the classifier, use sklearn’s GridSearchCV. [hint: don’t\n",
        "overdo it at the beginning, since runtime might go up fast] You are supposed to experiment with the\n",
        "following hyperparameters1:\n",
        "a. Penalty (Regularization)\n",
        "b. Solver\n",
        "c. Experiment with parameters of the Vectorizer (not required, highly advised)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 352,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 353,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[('TfidfVect', TfidfVectorizer()),\n",
              "                                       ('LR', LogisticRegression())]),\n",
              "             param_grid={'LR__penalty': ['l1', 'l2'],\n",
              "                         'LR__solver': ['liblinear', 'saga'],\n",
              "                         'TfidfVect__ngram_range': [(1, 1), (1, 2)],\n",
              "                         'TfidfVect__norm': ['l1', 'l2']},\n",
              "             verbose=1)"
            ]
          },
          "execution_count": 353,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# text_LR = Pipeline([('CV', CountVectorizer()),('Tfidf', TfidfTransformer()),('LR', LogisticRegression())])\n",
        "text_LR = Pipeline([\n",
        "    ('TfidfVect', TfidfVectorizer()),\n",
        "    ('LR', LogisticRegression())\n",
        "])\n",
        "\n",
        "param_grid = {'LR__penalty': ['l1', 'l2'],\n",
        "               'LR__solver': ['liblinear', 'saga'],\n",
        "               'TfidfVect__ngram_range': [(1, 1), (1, 2)],\n",
        "               'TfidfVect__norm': ['l1', 'l2']}\n",
        "\n",
        "gs_LR = GridSearchCV(text_LR, param_grid, cv=5, verbose=1)\n",
        "gs_LR.fit(sub_train_set['text'], y_train_dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print out the first 5 best params. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 354,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_LR__penalty</th>\n",
              "      <th>param_LR__solver</th>\n",
              "      <th>param_TfidfVect__ngram_range</th>\n",
              "      <th>param_TfidfVect__norm</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split3_test_score</th>\n",
              "      <th>split4_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3.351609</td>\n",
              "      <td>0.256262</td>\n",
              "      <td>0.280216</td>\n",
              "      <td>0.070384</td>\n",
              "      <td>l2</td>\n",
              "      <td>saga</td>\n",
              "      <td>(1, 1)</td>\n",
              "      <td>l2</td>\n",
              "      <td>{'LR__penalty': 'l2', 'LR__solver': 'saga', 'T...</td>\n",
              "      <td>0.964583</td>\n",
              "      <td>0.961111</td>\n",
              "      <td>0.958565</td>\n",
              "      <td>0.963426</td>\n",
              "      <td>0.965972</td>\n",
              "      <td>0.962731</td>\n",
              "      <td>0.002623</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>9.400893</td>\n",
              "      <td>0.510846</td>\n",
              "      <td>0.525816</td>\n",
              "      <td>0.009730</td>\n",
              "      <td>l2</td>\n",
              "      <td>saga</td>\n",
              "      <td>(1, 2)</td>\n",
              "      <td>l2</td>\n",
              "      <td>{'LR__penalty': 'l2', 'LR__solver': 'saga', 'T...</td>\n",
              "      <td>0.963889</td>\n",
              "      <td>0.961343</td>\n",
              "      <td>0.959259</td>\n",
              "      <td>0.963426</td>\n",
              "      <td>0.963426</td>\n",
              "      <td>0.962269</td>\n",
              "      <td>0.001745</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>223.140502</td>\n",
              "      <td>7.983467</td>\n",
              "      <td>1.097650</td>\n",
              "      <td>0.161085</td>\n",
              "      <td>l2</td>\n",
              "      <td>liblinear</td>\n",
              "      <td>(1, 2)</td>\n",
              "      <td>l2</td>\n",
              "      <td>{'LR__penalty': 'l2', 'LR__solver': 'liblinear...</td>\n",
              "      <td>0.962731</td>\n",
              "      <td>0.960648</td>\n",
              "      <td>0.958796</td>\n",
              "      <td>0.962963</td>\n",
              "      <td>0.962731</td>\n",
              "      <td>0.961574</td>\n",
              "      <td>0.001624</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>108.429751</td>\n",
              "      <td>109.590767</td>\n",
              "      <td>0.378309</td>\n",
              "      <td>0.145460</td>\n",
              "      <td>l2</td>\n",
              "      <td>liblinear</td>\n",
              "      <td>(1, 1)</td>\n",
              "      <td>l2</td>\n",
              "      <td>{'LR__penalty': 'l2', 'LR__solver': 'liblinear...</td>\n",
              "      <td>0.962500</td>\n",
              "      <td>0.960417</td>\n",
              "      <td>0.957639</td>\n",
              "      <td>0.961806</td>\n",
              "      <td>0.964583</td>\n",
              "      <td>0.961389</td>\n",
              "      <td>0.002307</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2.898128</td>\n",
              "      <td>0.263738</td>\n",
              "      <td>0.243447</td>\n",
              "      <td>0.041382</td>\n",
              "      <td>l2</td>\n",
              "      <td>saga</td>\n",
              "      <td>(1, 1)</td>\n",
              "      <td>l1</td>\n",
              "      <td>{'LR__penalty': 'l2', 'LR__solver': 'saga', 'T...</td>\n",
              "      <td>0.937269</td>\n",
              "      <td>0.936806</td>\n",
              "      <td>0.938889</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.931481</td>\n",
              "      <td>0.936389</td>\n",
              "      <td>0.002550</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
              "13       3.351609      0.256262         0.280216        0.070384   \n",
              "15       9.400893      0.510846         0.525816        0.009730   \n",
              "11     223.140502      7.983467         1.097650        0.161085   \n",
              "9      108.429751    109.590767         0.378309        0.145460   \n",
              "12       2.898128      0.263738         0.243447        0.041382   \n",
              "\n",
              "   param_LR__penalty param_LR__solver param_TfidfVect__ngram_range  \\\n",
              "13                l2             saga                       (1, 1)   \n",
              "15                l2             saga                       (1, 2)   \n",
              "11                l2        liblinear                       (1, 2)   \n",
              "9                 l2        liblinear                       (1, 1)   \n",
              "12                l2             saga                       (1, 1)   \n",
              "\n",
              "   param_TfidfVect__norm                                             params  \\\n",
              "13                    l2  {'LR__penalty': 'l2', 'LR__solver': 'saga', 'T...   \n",
              "15                    l2  {'LR__penalty': 'l2', 'LR__solver': 'saga', 'T...   \n",
              "11                    l2  {'LR__penalty': 'l2', 'LR__solver': 'liblinear...   \n",
              "9                     l2  {'LR__penalty': 'l2', 'LR__solver': 'liblinear...   \n",
              "12                    l1  {'LR__penalty': 'l2', 'LR__solver': 'saga', 'T...   \n",
              "\n",
              "    split0_test_score  split1_test_score  split2_test_score  \\\n",
              "13           0.964583           0.961111           0.958565   \n",
              "15           0.963889           0.961343           0.959259   \n",
              "11           0.962731           0.960648           0.958796   \n",
              "9            0.962500           0.960417           0.957639   \n",
              "12           0.937269           0.936806           0.938889   \n",
              "\n",
              "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
              "13           0.963426           0.965972         0.962731        0.002623   \n",
              "15           0.963426           0.963426         0.962269        0.001745   \n",
              "11           0.962963           0.962731         0.961574        0.001624   \n",
              "9            0.961806           0.964583         0.961389        0.002307   \n",
              "12           0.937500           0.931481         0.936389        0.002550   \n",
              "\n",
              "    rank_test_score  \n",
              "13                1  \n",
              "15                2  \n",
              "11                3  \n",
              "9                 4  \n",
              "12                5  "
            ]
          },
          "execution_count": 354,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LR_df = pd.DataFrame.from_dict(gs_LR.cv_results_)\n",
        "LR_df.sort_values(by=[\"rank_test_score\"]).iloc[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Model and Error Analysis\n",
        "Report the hyperparameter combination for your best-performing model on the test set.\n",
        "What is the advantage of grid search cross-validation? Use a confusion matrix to do your error analysis and summarize your answers in your report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 355,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "mean_fit_time                                                            3.351609\n",
              "std_fit_time                                                             0.256262\n",
              "mean_score_time                                                          0.280216\n",
              "std_score_time                                                           0.070384\n",
              "param_LR__penalty                                                              l2\n",
              "param_LR__solver                                                             saga\n",
              "param_TfidfVect__ngram_range                                               (1, 1)\n",
              "param_TfidfVect__norm                                                          l2\n",
              "params                          {'LR__penalty': 'l2', 'LR__solver': 'saga', 'T...\n",
              "split0_test_score                                                        0.964583\n",
              "split1_test_score                                                        0.961111\n",
              "split2_test_score                                                        0.958565\n",
              "split3_test_score                                                        0.963426\n",
              "split4_test_score                                                        0.965972\n",
              "mean_test_score                                                          0.962731\n",
              "std_test_score                                                           0.002623\n",
              "rank_test_score                                                                 1\n",
              "Name: 13, dtype: object"
            ]
          },
          "execution_count": 355,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LR_df.sort_values(by=[\"rank_test_score\"]).iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 356,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'LR__penalty': 'l2',\n",
              " 'LR__solver': 'saga',\n",
              " 'TfidfVect__ngram_range': (1, 1),\n",
              " 'TfidfVect__norm': 'l2'}"
            ]
          },
          "execution_count": 356,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LR_df.sort_values(by=[\"rank_test_score\"]).iloc[0]['params']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 357,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('TfidfVect', TfidfVectorizer()),\n",
              "                ('LR', LogisticRegression(solver='saga'))])"
            ]
          },
          "execution_count": 357,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_model = Pipeline([\n",
        "    # ('CV', CountVectorizer(ngram_range=(1,1))),\n",
        "    # ('Tfidf', TfidfTransformer(norm='l2')), change due to eli5\n",
        "    ('TfidfVect', TfidfVectorizer(ngram_range=(1,1), norm='l2')),\n",
        "    ('LR', LogisticRegression(penalty='l2', solver='saga'))\n",
        "])\n",
        "\n",
        "best_model.fit(sub_train_set['text'], y_train_dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0,  0,  0, ..., 26, 26, 26])"
            ]
          },
          "execution_count": 358,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = best_model.predict(sub_test_set['text'])\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Congusion matrix for all classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 359,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[185,   0,   0,   0,   0,   0,  15,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0],\n",
              "        [  0, 186,   0,   0,  10,   0,   3,   0,   0,   0,   0,   1,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0],\n",
              "        [  0,   0, 200,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0],\n",
              "        [  0,   0,   0, 194,   3,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   2,   1,   0,\n",
              "           0],\n",
              "        [  0,   6,   0,   0, 188,   0,   1,   0,   0,   0,   0,   3,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   1,   0,   0,   1,   0,\n",
              "           0],\n",
              "        [  0,   0,   0,   0,   1, 192,   0,   0,   1,   0,   0,   6,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0],\n",
              "        [  3,   0,   0,   0,   0,   0, 190,   0,   0,   0,   0,   5,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   2,   0,\n",
              "           0],\n",
              "        [  0,   1,   0,   0,   0,   0,   0, 196,   0,   0,   0,   1,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   2,   0,\n",
              "           0],\n",
              "        [  0,   0,   0,   0,   0,   0,   1,   0, 196,   0,   0,   3,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0],\n",
              "        [  0,   0,   0,   0,   0,   0,   1,   0,   0, 194,   0,   5,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0],\n",
              "        [  0,   0,   0,   0,   1,   0,   0,   0,   0,   0, 194,   5,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0],\n",
              "        [  0,   0,   0,   0,   0,   0,   1,   0,   0,   0,   0, 195,   0,\n",
              "           0,   0,   0,   0,   0,   0,   2,   0,   2,   0,   0,   0,   0,\n",
              "           0],\n",
              "        [  0,   0,   0,   0,   0,   0,   1,   0,   0,   0,   0,   2, 187,\n",
              "           0,   0,   1,   0,   0,   0,   1,   0,   1,   0,   0,   7,   0,\n",
              "           0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
              "         199,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   2,   0,\n",
              "           0, 198,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0],\n",
              "        [  0,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0, 194,   0,   0,   0,   0,   0,   3,   0,   0,   0,   0,\n",
              "           0],\n",
              "        [  0,   0,   0,   0,   0,   0,   1,   0,   0,   0,   0,   2,   0,\n",
              "           0,   0,   0, 197,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0],\n",
              "        [  0,   0,   0,   0,   0,   0,   3,   0,   0,   0,   0,   3,   0,\n",
              "           0,   0,   0,   0, 194,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0],\n",
              "        [  0,   0,   0,   0,   0,   0,   3,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0, 197,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0],\n",
              "        [  0,   0,   0,   0,   0,   0,   1,   0,   0,   0,   0,   1,   0,\n",
              "           0,   0,   0,   0,   0,   0, 198,   0,   0,   0,   0,   0,   0,\n",
              "           0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0, 200,   0,   0,   0,   0,   0,\n",
              "           0],\n",
              "        [  0,   0,   0,   0,   0,   0,   1,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0, 194,   4,   0,   1,   0,\n",
              "           0],\n",
              "        [  0,   0,   0,   0,   1,   0,   0,   0,   0,   0,   0,   2,   0,\n",
              "           0,   0,   0,   1,   0,   0,   0,   0,   2, 191,   0,   3,   0,\n",
              "           0],\n",
              "        [  0,   0,   0,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 198,   0,   0,\n",
              "           0],\n",
              "        [  1,   0,   0,   0,   0,   0,  14,   0,   0,   0,   0,   5,   0,\n",
              "           0,   0,   0,   1,   0,   0,   1,   0,   0,   0,   0, 178,   0,\n",
              "           0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 200,\n",
              "           0],\n",
              "        [  0,   0,   0,   0,   0,   0,   1,   0,   0,   0,   0,   1,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         198]]),\n",
              " (27, 27))"
            ]
          },
          "execution_count": 359,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm, cm.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Class Metrics\n",
        "We can have precision score, recall score and f1 score on each class. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 360,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ang       0.98      0.93      0.95       200\n",
            "         bar       0.95      0.93      0.94       200\n",
            "         bod       1.00      1.00      1.00       200\n",
            "         dan       0.99      0.97      0.98       200\n",
            "         deu       0.92      0.94      0.93       200\n",
            "         dsb       1.00      0.96      0.98       200\n",
            "         eng       0.80      0.95      0.87       200\n",
            "         hat       1.00      0.98      0.99       200\n",
            "         hrv       0.99      0.98      0.99       200\n",
            "         hun       1.00      0.97      0.98       200\n",
            "         jav       1.00      0.97      0.98       200\n",
            "         jpn       0.80      0.97      0.88       200\n",
            "         kab       1.00      0.94      0.97       200\n",
            "         kbd       1.00      0.99      1.00       200\n",
            "         kor       1.00      0.99      0.99       200\n",
            "         ksh       0.99      0.97      0.98       200\n",
            "         lad       0.99      0.98      0.99       200\n",
            "         mar       1.00      0.97      0.98       200\n",
            "         mkd       1.00      0.98      0.99       200\n",
            "         mwl       0.98      0.99      0.99       200\n",
            "         nav       1.00      1.00      1.00       200\n",
            "      nds-nl       0.96      0.97      0.96       200\n",
            "         nld       0.98      0.95      0.97       200\n",
            "         nob       0.99      0.99      0.99       200\n",
            "         pcd       0.91      0.89      0.90       200\n",
            "         swe       1.00      1.00      1.00       200\n",
            "         urd       1.00      0.99      0.99       200\n",
            "\n",
            "    accuracy                           0.97      5400\n",
            "   macro avg       0.97      0.97      0.97      5400\n",
            "weighted avg       0.97      0.97      0.97      5400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report \n",
        "\n",
        "report = classification_report(\n",
        "    y_test, y_pred, \n",
        "    target_names=le_fitted.inverse_transform([i for i in range(len(subset_labels))]),\n",
        "    )\n",
        "\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Diagonal Elements:\n",
        "\n",
        "Watching on the top-left to the bottom-right elements, all of these diagonal elements are close to 200, the total number of samples each label in test set. This means the true positive values for each label is relatively high, indicating that the classifier performs well on most classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Non-Diagonal Elements:\n",
        "\n",
        "However, there are also non-diagonal elements, which are errors (wrong predictions). The maximum number a class is misclassified is 15, which is the number 'ang' texts are misclassified as 'eng' texts. Another big number is 14, which shows the count of 'pcd' samples are misclassified as 'eng' texts. To be noticed, we can find that there are also some misclassifications from 'eng' to 'ang' or 'pcd' languages, though a bit fewer. These results indicate that there are some confusion between these pairs of languages. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(15, 14)"
            ]
          },
          "execution_count": 376,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cm[0][6], cm[24][6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array(['ang', 'eng'], dtype=object), array(['pcd', 'eng'], dtype=object))"
            ]
          },
          "execution_count": 377,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "le_fitted.inverse_transform([0, 6]), le_fitted.inverse_transform([24, 6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Explaination\n",
        "\"Now that you have your best model, it’s time to dive deep into understanding how the model makes predictions. \n",
        "It is important that we can explain and visualize our models to improve task performance. Explainable models help \n",
        "characterize model fairness, transparency, and outcomes. Let's try to understand what our best-performing logistic \n",
        "regression classification model has learned. **Generate a feature importance table for the top ten features \n",
        "(please have the features named) for the languages English, Swedish, Norwegian, and Japanese. What is \n",
        "more important, extra features or the outputs of the vectorizer, discuss**.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {},
      "outputs": [],
      "source": [
        "# languages = ['eng', 'swe', 'nob']\n",
        "languages = ['eng', 'swe', 'nob', 'jpn']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 364,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: eli5 in /Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages (0.13.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages (from eli5) (0.9.0)\n",
            "Requirement already satisfied: attrs>17.1.0 in /Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages (from eli5) (22.1.0)\n",
            "Requirement already satisfied: jinja2>=3.0.0 in /Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages (from eli5) (3.0.3)\n",
            "Requirement already satisfied: scipy in /Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages (from eli5) (1.7.3)\n",
            "Requirement already satisfied: six in /Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages (from eli5) (1.16.0)\n",
            "Requirement already satisfied: graphviz in /Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages (from eli5) (0.20.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages (from eli5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages (from eli5) (1.21.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages (from jinja2>=3.0.0->eli5) (2.1.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages (from scikit-learn>=0.20->eli5) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/nanfangwuyu/opt/anaconda3/envs/CS-NLP/lib/python3.7/site-packages (from scikit-learn>=0.20->eli5) (3.1.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install eli5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('TfidfVect', TfidfVectorizer()),\n",
              "                ('LR', LogisticRegression(solver='saga'))])"
            ]
          },
          "execution_count": 379,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "best_model = Pipeline([\n",
        "    # ('CV', CountVectorizer(ngram_range=(1,1))),\n",
        "    # ('Tfidf', TfidfTransformer(norm='l2')), change due to eli5\n",
        "    ('TfidfVect', TfidfVectorizer(ngram_range=(1,1), norm='l2')),\n",
        "    ('LR', LogisticRegression(penalty='l2', solver='saga'))\n",
        "])\n",
        "\n",
        "best_model.fit(sub_train_set['text'], y_train_dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "    \n",
              "        <table class=\"eli5-weights-wrapper\" style=\"border-collapse: collapse; border: none; margin-bottom: 1.5em;\">\n",
              "            <tr>\n",
              "                \n",
              "                    <td style=\"padding: 0.5em; border: 1px solid black; text-align: center;\">\n",
              "                        <b>\n",
              "    \n",
              "        y=eng\n",
              "    \n",
              "</b>\n",
              "\n",
              "top features\n",
              "                    </td>\n",
              "                \n",
              "                    <td style=\"padding: 0.5em; border: 1px solid black; text-align: center;\">\n",
              "                        <b>\n",
              "    \n",
              "        y=swe\n",
              "    \n",
              "</b>\n",
              "\n",
              "top features\n",
              "                    </td>\n",
              "                \n",
              "                    <td style=\"padding: 0.5em; border: 1px solid black; text-align: center;\">\n",
              "                        <b>\n",
              "    \n",
              "        y=nob\n",
              "    \n",
              "</b>\n",
              "\n",
              "top features\n",
              "                    </td>\n",
              "                \n",
              "                    <td style=\"padding: 0.5em; border: 1px solid black; text-align: center;\">\n",
              "                        <b>\n",
              "    \n",
              "        y=jpn\n",
              "    \n",
              "</b>\n",
              "\n",
              "top features\n",
              "                    </td>\n",
              "                \n",
              "            </tr>\n",
              "            <tr>\n",
              "                \n",
              "                    \n",
              "                        <td style=\"padding: 0px; border: 1px solid black; vertical-align: top;\">\n",
              "                            \n",
              "                                \n",
              "                                    \n",
              "                                    \n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; width: 100%;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
              "                    Weight<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 82.57%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +7.963\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        the\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 85.46%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +6.145\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        and\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 86.71%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +5.407\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        in\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 88.97%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +4.143\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        was\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 89.23%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +4.005\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        of\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 90.62%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +3.285\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        to\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 91.35%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.929\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        with\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 91.66%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.780\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        by\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 91.66%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.777\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        he\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 91.69%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.762\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        as\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 91.69%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 8879 more positive &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 251740 more negative &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "                                \n",
              "                            \n",
              "                        </td>\n",
              "                    \n",
              "                        <td style=\"padding: 0px; border: 1px solid black; vertical-align: top;\">\n",
              "                            \n",
              "                                \n",
              "                                    \n",
              "                                    \n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; width: 100%;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
              "                    Weight<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 80.68%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +9.226\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        och\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 81.34%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +8.776\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        är\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 85.65%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +6.029\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        av\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 89.24%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +3.999\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        till\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 89.25%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +3.993\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        som\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 89.37%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +3.927\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        den\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 90.81%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +3.190\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        att\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 90.99%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +3.103\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        för\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 91.31%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.945\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        på\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 91.47%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.867\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        finns\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 91.47%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 7460 more positive &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 253159 more negative &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "                                \n",
              "                            \n",
              "                        </td>\n",
              "                    \n",
              "                        <td style=\"padding: 0px; border: 1px solid black; vertical-align: top;\">\n",
              "                            \n",
              "                                \n",
              "                                    \n",
              "                                    \n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; width: 100%;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
              "                    Weight<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +9.693\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        og\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 83.54%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +7.339\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        av\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 86.30%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +5.647\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        ble\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 86.56%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +5.495\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        er\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 87.58%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +4.910\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        til\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 87.59%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +4.903\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        som\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 88.92%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +4.171\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        på\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 89.49%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +3.864\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        det\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 90.72%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +3.236\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        med\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 91.55%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.829\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        for\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 91.55%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 11731 more positive &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 248888 more negative &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "                                \n",
              "                            \n",
              "                        </td>\n",
              "                    \n",
              "                        <td style=\"padding: 0px; border: 1px solid black; vertical-align: top;\">\n",
              "                            \n",
              "                                \n",
              "                                    \n",
              "                                    \n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; width: 100%;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
              "                    Weight<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 84.84%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +6.524\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        また\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 90.45%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +3.369\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        しかし\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 91.32%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.940\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        なお\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 92.69%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.303\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        その後\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 92.69%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 12620 more positive &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 93.35%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 247999 more negative &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 93.35%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -2.009\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        and\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 92.53%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -2.374\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        se\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 92.15%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -2.548\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        is\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 91.51%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -2.850\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        en\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 89.84%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -3.683\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        in\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 87.48%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -4.962\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        de\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "                                \n",
              "                            \n",
              "                        </td>\n",
              "                    \n",
              "                \n",
              "            </tr>\n",
              "        </table>\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 381,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import eli5\n",
        "\n",
        "eli5.show_weights(best_model, top = 10, target_names = le_fitted.inverse_transform([i for i in range(len(subset_labels))]), targets = languages) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 367,
      "metadata": {},
      "outputs": [],
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ablation Study\n",
        "Lastly, you will condact a small ablation study. First, choose the two languages for which the classifier worked best. \n",
        "Next, re-fit the best working model several times, each time reducing the number of characters per instance in the \n",
        "training set (1. All characters, 2. 500 characters, 3. 100 characters). How does the ablation affect the \n",
        "performance of the classifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 368,
      "metadata": {},
      "outputs": [],
      "source": [
        "report = classification_report(\n",
        "    y_test, y_pred, \n",
        "    target_names=le_fitted.inverse_transform([i for i in range(len(subset_labels))]),\n",
        "    output_dict=True\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 369,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'precision': 0.971898264971494,\n",
              " 'recall': 0.9690740740740741,\n",
              " 'f1-score': 0.969793693483971,\n",
              " 'support': 5400}"
            ]
          },
          "execution_count": 369,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "report.pop('accuracy')\n",
        "report.pop('macro avg') \n",
        "report.pop('weighted avg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 370,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['bod', 'nav', 'swe', 'kbd', 'kor', 'urd', 'mkd', 'hat', 'hun', 'jav', 'mar', 'dsb', 'kab', 'hrv', 'ksh', 'nob', 'lad', 'dan', 'mwl', 'nld', 'ang', 'nds-nl', 'bar', 'deu', 'pcd', 'jpn', 'eng']\n",
            "{'bod': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 200}, 'nav': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 200}, 'swe': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 200}, 'kbd': {'precision': 1.0, 'recall': 0.995, 'f1-score': 0.9974937343358395, 'support': 200}, 'kor': {'precision': 1.0, 'recall': 0.99, 'f1-score': 0.9949748743718593, 'support': 200}, 'urd': {'precision': 1.0, 'recall': 0.99, 'f1-score': 0.9949748743718593, 'support': 200}, 'mkd': {'precision': 1.0, 'recall': 0.985, 'f1-score': 0.9924433249370278, 'support': 200}, 'hat': {'precision': 1.0, 'recall': 0.98, 'f1-score': 0.98989898989899, 'support': 200}, 'hun': {'precision': 1.0, 'recall': 0.97, 'f1-score': 0.9847715736040609, 'support': 200}, 'jav': {'precision': 1.0, 'recall': 0.97, 'f1-score': 0.9847715736040609, 'support': 200}, 'mar': {'precision': 1.0, 'recall': 0.97, 'f1-score': 0.9847715736040609, 'support': 200}, 'dsb': {'precision': 1.0, 'recall': 0.96, 'f1-score': 0.9795918367346939, 'support': 200}, 'kab': {'precision': 1.0, 'recall': 0.935, 'f1-score': 0.9664082687338501, 'support': 200}, 'hrv': {'precision': 0.9949238578680203, 'recall': 0.98, 'f1-score': 0.9874055415617129, 'support': 200}, 'ksh': {'precision': 0.9948717948717949, 'recall': 0.97, 'f1-score': 0.9822784810126582, 'support': 200}, 'nob': {'precision': 0.99, 'recall': 0.99, 'f1-score': 0.99, 'support': 200}, 'lad': {'precision': 0.9899497487437185, 'recall': 0.985, 'f1-score': 0.9874686716791979, 'support': 200}, 'dan': {'precision': 0.9897959183673469, 'recall': 0.97, 'f1-score': 0.9797979797979798, 'support': 200}, 'mwl': {'precision': 0.9801980198019802, 'recall': 0.99, 'f1-score': 0.9850746268656716, 'support': 200}, 'nld': {'precision': 0.9794871794871794, 'recall': 0.955, 'f1-score': 0.9670886075949366, 'support': 200}, 'ang': {'precision': 0.9788359788359788, 'recall': 0.925, 'f1-score': 0.9511568123393317, 'support': 200}, 'nds-nl': {'precision': 0.9556650246305419, 'recall': 0.97, 'f1-score': 0.9627791563275433, 'support': 200}, 'bar': {'precision': 0.9489795918367347, 'recall': 0.93, 'f1-score': 0.9393939393939393, 'support': 200}, 'deu': {'precision': 0.9215686274509803, 'recall': 0.94, 'f1-score': 0.9306930693069307, 'support': 200}, 'pcd': {'precision': 0.9128205128205128, 'recall': 0.89, 'f1-score': 0.9012658227848102, 'support': 200}, 'jpn': {'precision': 0.8024691358024691, 'recall': 0.975, 'f1-score': 0.8803611738148984, 'support': 200}, 'eng': {'precision': 0.8016877637130801, 'recall': 0.95, 'f1-score': 0.8695652173913043, 'support': 200}}\n"
          ]
        }
      ],
      "source": [
        "ranked_labels = sorted(report, key=lambda x: (report[x]['precision'], report[x]['recall'], report[x]['f1-score']), reverse=True)\n",
        "print(ranked_labels)\n",
        "ranked_report = {label: report[label] for label in ranked_labels}\n",
        "print(ranked_report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The two languages for which the classifier worked best are: 'bod', 'nav'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 371,
      "metadata": {},
      "outputs": [],
      "source": [
        "ablation_train = sub_train_set[sub_train_set['label'].isin(('bod', 'nav'))]\n",
        "ablation_test = sub_test_set[sub_test_set['label'].isin(('bod', 'nav'))]\n",
        "ablation_y_train = le_fitted.transform(ablation_train['label'])\n",
        "ablation_y_test = le_fitted.transform(ablation_test['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 372,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy when length is original: 1.0\n",
            "Accuracy when length is 500: 1.0\n",
            "Accuracy when length is 100: 1.0\n",
            "Accuracy when length is 10: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "limits = [None, 500, 100, 10]\n",
        "\n",
        "\n",
        "\n",
        "for limit in limits:\n",
        "\n",
        "    best_model = Pipeline([\n",
        "    ('TfidfVect', TfidfVectorizer(ngram_range=(1,1), norm='l2')),\n",
        "    ('LR', LogisticRegression(penalty='l2', solver='saga'))\n",
        "])\n",
        "    \n",
        "    if limit == None:\n",
        "        ablation_X_train = ablation_train['text']\n",
        "    else:\n",
        "        ablation_X_train = [text[:limit] for text in ablation_train['text']]\n",
        "        \n",
        "    \n",
        "    best_model.fit(ablation_X_train, ablation_y_train)\n",
        "\n",
        "    ablation_y_pred = best_model.predict(ablation_test['text'])\n",
        "\n",
        "    print(\"Accuracy when length is {}:\".format(\"original\" if limit == None else limit), accuracy_score(ablation_y_test, ablation_y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 373,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text     ༡༩༣༥ལོར་ཞང་པོ་དང་ལྷན་དུ་ནན་ཅིང་དུ་སློབ་སྦྱོང་མ...\n",
            "label                                                  bod\n",
            "Name: 17600, dtype: object\n",
            "text     Chąąneiłhizii éí chéłchaaʼ dah yikahjįʼ atah d...\n",
            "label                                                  nav\n",
            "Name: 115999, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(ablation_train.iloc[0])\n",
        "print(ablation_train.iloc[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a result, the ablation has no influence on the accuracy. This is probably because these two languages ( 'nav' and 'bod' ) use compeletly different vocabulary, they are very easy to distinguish by the best model. In this case, length of the text is slightly influencial. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, if we choose other two languages like 'nob' and 'swe', when the length reduces to 10, there is a obvious drop in accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 374,
      "metadata": {},
      "outputs": [],
      "source": [
        "ablation_train = sub_train_set[sub_train_set['label'].isin(('nob', 'swe'))]\n",
        "ablation_test = sub_test_set[sub_test_set['label'].isin(('nob', 'swe'))]\n",
        "ablation_y_train = le_fitted.transform(ablation_train['label'])\n",
        "ablation_y_test = le_fitted.transform(ablation_test['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy when length is original: 1.0\n",
            "Accuracy when length is 500: 1.0\n",
            "Accuracy when length is 100: 1.0\n",
            "Accuracy when length is 10: 0.9225\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "limits = [None, 500, 100, 10]\n",
        "\n",
        "\n",
        "\n",
        "for limit in limits:\n",
        "\n",
        "    best_model = Pipeline([\n",
        "    ('TfidfVect', TfidfVectorizer(ngram_range=(1,1), norm='l2')),\n",
        "    ('LR', LogisticRegression(penalty='l2', solver='saga'))\n",
        "])\n",
        "    \n",
        "    if limit == None:\n",
        "        ablation_X_train = ablation_train['text']\n",
        "    else:\n",
        "        ablation_X_train = [text[:limit] for text in ablation_train['text']]\n",
        "        \n",
        "    \n",
        "    best_model.fit(ablation_X_train, ablation_y_train)\n",
        "\n",
        "    ablation_y_pred = best_model.predict(ablation_test['text'])\n",
        "\n",
        "    print(\"Accuracy when length is {}:\".format(\"original\" if limit == None else limit), accuracy_score(ablation_y_test, ablation_y_pred))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dyslexia-chinese",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
